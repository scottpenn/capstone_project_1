{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first(n):\n",
    "    return sum(x for x in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_first(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "n = 5\n",
    "for x in range(n):\n",
    "    sum += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: A Shallow Dive into Deep Learning\n",
    "\n",
    "To put what I've learned in the Deep Learning Unit into practice, I will attempt to improve upon my Gradient Boosting model from the Machine Learning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for reading in the data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial setup is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv('Resources/tennis_clean/atp_top_100_matches.csv',\n",
    "                      index_col=['player_id', 'tournament_id', 'match_id'],\n",
    "                      parse_dates=['tournament_date'], low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches['points_diff'] = matches.points_won - (matches.points - matches.points_won)\n",
    "matches['ranking_points_diff'] = matches.ranking_points - matches.opponent_ranking_points\n",
    "matches['rank_diff'] = matches.opponent_rank - matches.player_rank\n",
    "matches['height_diff'] = matches.player_height - matches.opponent_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_matches = matches[(matches['tournament_date'] > '2009') & (matches['tournament_date'] < '2019')]\n",
    "test_matches = matches[matches['tournament_date'] > '2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['ranking_points_diff', 'rank_diff',\n",
    "       'recent_first_serve_percentage', 'recent_break_points_save_percentage', 'recent_service_points_won_percentage', \n",
    "       'recent_return_points_won_percentage',\n",
    "       'recent_first_serves_won_percentage', 'recent_second_serves_won_percentage',\n",
    "       'recent_first_serve_return_points_won_percentage',\n",
    "       'recent_second_serve_return_points_won_percentage', \n",
    "       'recent_break_points_won_percentage',\n",
    "       'recent_points_won_percentage',\n",
    "       'past_year_first_serve_percentage', 'past_year_break_points_save_percentage', \n",
    "       'past_year_service_points_won_percentage', \n",
    "       'past_year_return_points_won_percentage',\n",
    "       'past_year_first_serves_won_percentage', 'past_year_second_serves_won_percentage',\n",
    "       'past_year_first_serve_return_points_won_percentage',\n",
    "       'past_year_second_serve_return_points_won_percentage', \n",
    "       'past_year_break_points_won_percentage',\n",
    "       'past_year_points_won_percentage',\n",
    "       'career_first_serve_percentage', 'career_break_points_save_percentage', 'career_service_points_won_percentage', \n",
    "       'career_return_points_won_percentage',\n",
    "       'career_first_serves_won_percentage', 'career_second_serves_won_percentage',\n",
    "       'career_first_serve_return_points_won_percentage',\n",
    "       'career_second_serve_return_points_won_percentage', \n",
    "       'career_break_points_won_percentage',\n",
    "       'career_points_won_percentage', 'h2h', 'winrate', 'age_diff', 'result_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the neural network model, I will use the Keras library, which is built upon TensorFlow. Keras requires that the result column is in a categorical format, but it provides the function \"to_categorical\" for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train = recent_matches[COLUMNS].dropna().drop('result_value', axis=1)\n",
    "y_train = recent_matches[COLUMNS].dropna().result_value\n",
    "\n",
    "X_test = test_matches[COLUMNS].dropna().drop('result_value', axis=1)\n",
    "y_test = test_matches[COLUMNS].dropna().result_value\n",
    "\n",
    "y_train_cat = to_categorical(y_train) \n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must first be scaled to improve accuracy. The test data is not scaled independently, but according to the fit of the train data. After testing both the StandardScaler and the MinMaxScaler, the StandardScaler performs better and converges in fewer epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the neural network, I used a simple sequential model with just one hidden layer containing 100 nodes. I tested many different configurations, but adding layers and nodes did not improve the final test accuracy score. The Rectified Linear Unit or \"RELU\" activation function performs better than sigmoid activation functions like \"tanh\". The output layer must have 2 nodes, representing a win or a loss. The \"softmax\" activation function is also necessary for binary classification, as it converts the outputs to probabilities and ensures that they add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was compiled with the \"adam\" optimizer. I also chose the \"categorical_crossentropy\" loss function, a good default for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import accuracy\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the model, I used two callbacks provided by Keras. The EarlyStopping callback stops when the validation loss function does not improve after five epochs. The ModelCheckpoint callback saves the model with the best validation accuracy score. I experimented with splitting the training data to create an additional validation set, but setting the test data as the validation set optimizes for test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53676 samples, validate on 4996 samples\n",
      "Epoch 1/10\n",
      "53676/53676 [==============================] - 2s 29us/step - loss: 0.5614 - accuracy: 0.7059 - val_loss: 0.6138 - val_accuracy: 0.6585\n",
      "Epoch 2/10\n",
      "53676/53676 [==============================] - 1s 23us/step - loss: 0.5599 - accuracy: 0.7068 - val_loss: 0.6157 - val_accuracy: 0.6569\n",
      "Epoch 3/10\n",
      "53676/53676 [==============================] - 1s 24us/step - loss: 0.5590 - accuracy: 0.7091 - val_loss: 0.6143 - val_accuracy: 0.6587\n",
      "Epoch 4/10\n",
      "53676/53676 [==============================] - 2s 31us/step - loss: 0.5575 - accuracy: 0.7092 - val_loss: 0.6138 - val_accuracy: 0.6597\n",
      "Epoch 5/10\n",
      "53676/53676 [==============================] - 1s 26us/step - loss: 0.5559 - accuracy: 0.7108 - val_loss: 0.6176 - val_accuracy: 0.6569:\n",
      "Epoch 6/10\n",
      "53676/53676 [==============================] - 1s 27us/step - loss: 0.5548 - accuracy: 0.7122 - val_loss: 0.6206 - val_accuracy: 0.6577\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from keras.callbacks.callbacks import ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_cat, \n",
    "          epochs=10, \n",
    "          callbacks= [es], #mc], \n",
    "          batch_size=32, \n",
    "          validation_data=(X_test_scaled, y_test_cat), \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An evaluation of the best model shows a test accuracy of 66.5%. This outperforms any of my previous Machine Learning models, even after hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53676/53676 [==============================] - 0s 7us/step\n",
      "4996/4996 [==============================] - 0s 9us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7016729712486267, 0.6651321053504944)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('best_model.h5')\n",
    "_, train_accuracy = best_model.evaluate(X_train_scaled, y_train_cat)\n",
    "_, test_accuracy = best_model.evaluate(X_test_scaled, y_test_cat)\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not believe that the model has much room for improvement. For proof, I trained the model for 100 epochs, and plotted its history. As shown below, the test loss steadily increases after only a few epochs. Similarly, the test accuracy decreases after only a few epochs, then converges. Even at a training accuracy of 75%, the model is overfitting.\n",
    "\n",
    "The presence of upsets in the match data throw off any attempt at finding general patterns in the features. Increasing the training accuracy can only reduce the model's ability to generalize for the test data. To improve the model at this point, I would need to add new features that can generalize more effectively when encountering upsets in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". | .\n",
    "- | - \n",
    "![loss](epoch-loss.png) | ![loss](epoch-accuracy.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- Neural networks are not only useful for image and audio recognition. They can also be a powerful tool for classification.\n",
    "- Simple models work well for simple data. Complexity should be added only when needed.\n",
    "- Beware of overfitting. Longer training time does not mean better accuracy on unseen data.\n",
    "- Utilize callbacks to save the best model, and to stop training early.\n",
    "- If you're stuck, it might be time to look at the dataset's features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
